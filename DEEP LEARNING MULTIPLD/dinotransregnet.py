# -*- coding: utf-8 -*-
"""DinoTransRegNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l7F2N0okfVPPcmb3--KqPaBf3Yf0MDW1
"""

import h5py
import numpy as np
import os

dataset_dir = "/home/labadmin/Nino/Final/"
output_dir = "/home/labadmin/Nino/processed_data"
os.makedirs(output_dir, exist_ok=True)

# Iterate over all 24 subjects
for subject_id in range(1, 25):
    file_path = os.path.join(dataset_dir, f"subject_{subject_id}_noisy.mat")

    if not os.path.exists(file_path):
        print(f"Skipping {file_path}, file not found.")
        continue

    with h5py.File(file_path, 'r') as mat_file:
        data = mat_file['data_phantom_noisy']

        # Extract required datasets
        PWI_images = np.array(data['PWI_images'])  # Shape: (3, 60, 60, 72)
        CBF_groundtruth = np.array(data['CBF_groundtruth'])  # Shape: (60, 60, 72)
        ATT_groundtruth = np.array(data['ATT_groundtruth'])  # Shape: (60, 60, 72)

        # Rearrange PWI images to (60, 60, 72, 3) (channels-last format)
        PWI_images = np.moveaxis(PWI_images, 0, -1)  # Shape: (60, 60, 72, 3)

        # Save the reshaped arrays with subject identifier
        np.save(os.path.join(output_dir, f"PWI_images_subject_{subject_id}.npy"), PWI_images)
        np.save(os.path.join(output_dir, f"CBF_groundtruth_subject_{subject_id}.npy"), CBF_groundtruth)
        np.save(os.path.join(output_dir, f"ATT_groundtruth_subject_{subject_id}.npy"), ATT_groundtruth)

print("Preprocessing complete! Data saved in:", output_dir)

ls /home/labadmin/Nino/Final/

import numpy as np
import os

output_dir = "/home/labadmin/Nino/processed_data"

# Load a sample subject's data
subject_id = 1
PWI = np.load(os.path.join(output_dir, f"PWI_images_subject_{subject_id}.npy"))
CBF_gt = np.load(os.path.join(output_dir, f"CBF_groundtruth_subject_{subject_id}.npy"))
ATT_gt = np.load(os.path.join(output_dir, f"ATT_groundtruth_subject_{subject_id}.npy"))

print("PWI shape:", PWI.shape)  # Expected: (60, 60, 72, 3)
print("CBF Ground Truth shape:", CBF_gt.shape)  # Expected: (60, 60, 72)
print("ATT Ground Truth shape:", ATT_gt.shape)  # Expected: (60, 60, 72)

import h5py
import numpy as np
import os

# Define directories
dataset_dir = "/home/labadmin/Nino/Final/"
output_dir = "/home/labadmin/Nino/processed_data2"
os.makedirs(output_dir, exist_ok=True)

def normalize_pwi(images):
    """Normalize PWI images to [0,1] range."""
    images = images.astype(np.float32)
    return (images - images.min()) / (images.max() - images.min())



def normalize_groundtruth(gt):
    """Normalize ground truth to [0, 1] range."""
    gt = gt.astype(np.float32)
    gt_min = np.min(gt)
    gt_max = np.max(gt)
    if gt_max - gt_min == 0:
        return np.zeros_like(gt)  # avoid divide-by-zero if flat
    return (gt - gt_min) / (gt_max - gt_min)


# Iterate over all 24 subjects
for subject_id in range(1, 25):
    file_path = os.path.join(dataset_dir, f"subject_{subject_id}_noisy.mat")

    if not os.path.exists(file_path):
        print(f"Skipping {file_path}, file not found.")
        continue

    with h5py.File(file_path, 'r') as mat_file:
        data = mat_file['data_phantom_noisy']

        # Extract required datasets
        PWI_images = np.array(data['PWI_images'])  # Shape: (3, 60, 60, 72)
        CBF_groundtruth = np.array(data['CBF_groundtruth'])  # Shape: (60, 60, 72)
        ATT_groundtruth = np.array(data['ATT_groundtruth'])  # Shape: (60, 60, 72)

        # Rearrange PWI images to (60, 60, 72, 3) (channels-last format)
        PWI_images = np.moveaxis(PWI_images, 0, -1)  # Shape: (60, 60, 72, 3)

        # Normalize inputs and standardize ground truth
        PWI_images = normalize_pwi(PWI_images)
        CBF_groundtruth = normalize_groundtruth(CBF_groundtruth)
        ATT_groundtruth = normalize_groundtruth(ATT_groundtruth)

        # Save the processed arrays
        np.save(os.path.join(output_dir, f"PWI_images_subject_{subject_id}.npy"), PWI_images)
        np.save(os.path.join(output_dir, f"CBF_groundtruth_subject_{subject_id}.npy"), CBF_groundtruth)
        np.save(os.path.join(output_dir, f"ATT_groundtruth_subject_{subject_id}.npy"), ATT_groundtruth)

print("Data preparation complete! Normalized and saved in:", output_dir)

for subject_id in range(1, 25):
    pwi_path = os.path.join(output_dir, f"PWI_images_subject_{subject_id}.npy")
    cbf_path = os.path.join(output_dir, f"CBF_groundtruth_subject_{subject_id}.npy")
    att_path = os.path.join(output_dir, f"ATT_groundtruth_subject_{subject_id}.npy")

    if os.path.exists(pwi_path) and os.path.exists(cbf_path) and os.path.exists(att_path):
        print(f"Subject {subject_id}: Data loaded successfully.")
        print("PWI shape:", np.load(pwi_path).shape)
        print("CBF Ground Truth shape:", np.load(cbf_path).shape)
        print("ATT Ground Truth shape:", np.load(att_path).shape)
    else:
        print(f"Missing files for subject {subject_id}")

import os
import numpy as np
from sklearn.model_selection import train_test_split

# Directories
processed_dir = "/home/labadmin/Nino/processed_data2"
train_dir = "/home/labadmin/Nino/train_data"
test_dir = "/home/labadmin/Nino/test_data"

# Create train/test directories
os.makedirs(train_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Define subjects
all_subjects = list(range(1, 25))
train_subjects, test_subjects = train_test_split(all_subjects, test_size=5, random_state=42)

def move_files(subjects, subset_dir):
    for subject_id in subjects:
        for file_type in ["PWI_images", "CBF_groundtruth", "ATT_groundtruth"]:
            src_path = os.path.join(processed_dir, f"{file_type}_subject_{subject_id}.npy")
            dst_path = os.path.join(subset_dir, f"{file_type}_subject_{subject_id}.npy")

            if os.path.exists(src_path):
                os.rename(src_path, dst_path)  # Move file
            else:
                print(f"Missing file: {src_path}")

# Move files to respective directories
move_files(train_subjects, train_dir)
move_files(test_subjects, test_dir)

print("✅ Data split complete! Train and test sets created.")
print(f"Train subjects: {train_subjects}")
print(f"Test subjects: {test_subjects}")

pip install scipy torch torchvision numpy

import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader

# Define a PyTorch Dataset Class for Your 3D Data
class ASL3DDataset(Dataset):
    def __init__(self, data_dir, subject_list):
        """
        data_dir: Path to folder containing .npy files
        subject_list: List of subject IDs to load
        """
        self.data_dir = data_dir
        self.subject_list = subject_list

    def __len__(self):
        return len(self.subject_list)

    def __getitem__(self, idx):
        subject_id = self.subject_list[idx]

        #  Load preprocessed .npy files for this subject
        pwi = np.load(f"{self.data_dir}/PWI_images_subject_{subject_id}.npy")  # Shape: (3, D, H, W)
        cbf_gt = np.load(f"{self.data_dir}/CBF_groundtruth_subject_{subject_id}.npy")  # Shape: (D, H, W)
        att_gt = np.load(f"{self.data_dir}/ATT_groundtruth_subject_{subject_id}.npy")  # Shape: (D, H, W)

        pwi = np.transpose(pwi, (3, 0, 1, 2))  # Move channels to the first dimension
        #  Stack ground truth maps as channels: Shape -> (2, Depth, Height, Width)
        gt = np.stack([cbf_gt, att_gt], axis=0)

        #  Convert to PyTorch tensors
        pwi_tensor = torch.tensor(pwi, dtype=torch.float32)
        gt_tensor = torch.tensor(gt, dtype=torch.float32)

        return pwi_tensor, gt_tensor

#  Create DataLoaders for Training and Testing
train_data_dir = "/home/labadmin/Nino/train_data"
test_data_dir = "/home/labadmin/Nino/test_data"

train_subjects = [10, 14, 2, 22, 6, 3, 13, 16, 4, 5, 23, 18, 21, 24, 8, 11, 15, 20, 7]
test_subjects = [9, 17, 1, 19, 12]


train_dataset = ASL3DDataset(train_data_dir, train_subjects)
test_dataset = ASL3DDataset(test_data_dir, test_subjects)

train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

# ✅ Test if data loads correctly
for pwi, gt in train_loader:
    print("PWI shape:", pwi.shape)  # Expected: (batch_size, 3, D, H, W)
    print("GT shape:", gt.shape)    # Expected: (batch_size, 2, D, H, W)
    break



import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader

# ✅ 3D Patch Embedding Layer with Padding
class PatchEmbedding3D(nn.Module):
    def __init__(self, in_channels=3, patch_size=(8, 8, 8), embed_dim=256):  # Reduced embed_dim to 256
        super().__init__()
        self.patch_size = patch_size
        self.proj = nn.Conv3d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, D, H, W = x.shape
        pad_d = (self.patch_size[0] - D % self.patch_size[0]) % self.patch_size[0]
        pad_h = (self.patch_size[1] - H % self.patch_size[1]) % self.patch_size[1]
        pad_w = (self.patch_size[2] - W % self.patch_size[2]) % self.patch_size[2]
        # Pad in the order: (W_before, W_after, H_before, H_after, D_before, D_after)
        x = F.pad(x, (0, pad_w, 0, pad_h, 0, pad_d))
        x = self.proj(x)  # (B, embed_dim, D', H', W')
        x = x.flatten(2).transpose(1, 2)  # (B, Num_Patches, Embed_Dim)
        return x

class ViT3D(nn.Module):
    def __init__(self,
                 img_size=(60, 60, 72),
                 patch_size=(8, 8, 8),
                 in_channels=3,
                 embed_dim=256,
                 num_heads=6,
                 num_layers=6,
                 dropout_rate=0.3):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.embed_dim = embed_dim

        self.patch_embed = PatchEmbedding3D(in_channels, patch_size, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # Compute number of patches dynamically
        self.padded_d = (img_size[0] + patch_size[0] - 1) // patch_size[0]
        self.padded_h = (img_size[1] + patch_size[1] - 1) // patch_size[1]
        self.padded_w = (img_size[2] + patch_size[2] - 1) // patch_size[2]
        num_patches = self.padded_d * self.padded_h * self.padded_w

        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))
        self.dropout = nn.Dropout(p=dropout_rate)

        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout_rate)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # ✅ Conv3D Decoder
        self.decoder = nn.Sequential(
            nn.Conv3d(embed_dim, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(32, 2, kernel_size=1)  # Output: 2 channels (CBF & ATT)
        )

    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)  # (B, Num_Patches, Embed_Dim)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1 + Num_Patches, Embed_Dim)
        x = x + self.pos_embed[:, :x.size(1), :]
        x = self.dropout(x)
        x = self.transformer(x)

        # ✅ Discard CLS token and reshape to 3D grid
        x = x[:, 1:, :]  # (B, Num_Patches, Embed_Dim)
        x = x.transpose(1, 2).reshape(B, self.embed_dim, self.padded_d, self.padded_h, self.padded_w)

        # ✅ Decode to CBF & ATT maps
        x = self.decoder(x)  # (B, 2, D, H, W)
        predicted_cbf, predicted_att = x[:, 0], x[:, 1]
        return predicted_cbf, predicted_att


# ✅ 3D Vision Transformer Backbone (Modified for DINO)
class ViT3D(nn.Module):
    def __init__(self,
                 img_size=(60, 60, 72),
                 patch_size=(8, 8, 8),
                 in_channels=3,
                 embed_dim=256,           # Reduced embedding dimension
                 num_heads=6,             # Reduced number of heads accordingly
                 num_layers=6,            # Reduced number of transformer layers
                 dropout_rate=0.3):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size

        self.patch_embed = PatchEmbedding3D(in_channels, patch_size, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))

        # Compute number of patches dynamically
        padded_d = (img_size[0] + patch_size[0] - 1) // patch_size[0]
        padded_h = (img_size[1] + patch_size[1] - 1) // patch_size[1]
        padded_w = (img_size[2] + patch_size[2] - 1) // patch_size[2]
        num_patches = padded_d * padded_h * padded_w

        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))

        # Use dropout for regularization; here we use standard dropout instead of Dropout3d
        # because it works for any input dimensions in the transformer block.
        self.dropout = nn.Dropout(p=dropout_rate)

        # Define transformer encoder layers and stack them
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout_rate)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.head = nn.Linear(embed_dim, 2 * img_size[0] * img_size[1] * img_size[2])

    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)  # (B, Num_Patches, Embed_Dim)
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, 1 + Num_Patches, Embed_Dim)
        x = x + self.pos_embed[:, :x.size(1), :]

        # Apply dropout before transformer
        x = self.dropout(x)
        x = self.transformer(x)
        x = self.head(x[:, 0])  # Use the output corresponding to the CLS token
        x = x.view(B, 2, *self.img_size)  # Reshape to (B, 2, D, H, W)
        predicted_cbf, predicted_att = x[:, 0], x[:, 1]
        return predicted_cbf, predicted_att



# ✅ Training Function with a learning rate scheduler and (optionally) gradient clipping
def train_dino_3d(model, train_loader, epochs=2500, lr=1e-3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2500)
    #scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, step_size=30, gamma=0.5)

    criterion = nn.MSELoss()


    for epoch in range(epochs):
        model.train()
        total_loss_cbf = 0
        total_loss_att = 0

        for pwis, gt in train_loader:
            pwis, gt = pwis.to(device), gt.to(device)
            optimizer.zero_grad()

            predicted_cbf, predicted_att = model(pwis)

            loss_cbf = criterion(predicted_cbf, gt[:, 0])
            loss_att = criterion(predicted_att, gt[:, 1])
            total_loss = loss_cbf + loss_att

            total_loss.backward()
            # Optionally clip gradients if needed:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss_cbf += loss_cbf.item()
            total_loss_att += loss_att.item()

        avg_loss_cbf = total_loss_cbf / len(train_loader)
        avg_loss_att = total_loss_att / len(train_loader)
        print(f"Epoch {epoch+1}/{epochs}, Loss CBF: {avg_loss_cbf:.4f}, Loss ATT: {avg_loss_att:.4f}")
        scheduler.step()

        # Optional debug check
        with torch.no_grad():
            model.eval()
            preds = model(pwis)
            print("Predicted CBF shape:", preds[0].shape)
            print("Predicted ATT shape:", preds[1].shape)
            print("GT shape:", gt.shape)

# ✅ Instantiate Model and Train
dino_3d = ViT3D(
    img_size=(60, 60, 72),  # Target output shape
    patch_size=(8, 8, 8),
    in_channels=3,          # Adjust this if your input data differs (e.g. if it is one-channel, set to 1)
    embed_dim=252,          # Reduced embed_dim for a simpler model suited for a small dataset
    num_heads=6,
    num_layers=6,           # Fewer transformer layers for a small dataset
    dropout_rate=0.3        # Dropout rate (you may experiment with values, e.g., 0.2 or 0.3)
)

# Use a slightly larger batch size if possible for stability,
# but if data is very small, you can keep it lower.
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
train_dino_3d(dino_3d, train_loader)

import torch
import torch.nn.functional as F
from skimage.metrics import structural_similarity as ssim
import numpy as np



# Function to compute PSNR
def compute_psnr(pred, gt):
    mse = np.mean((pred - gt) ** 2)
    if mse == 0:
        return float('inf')  # Perfect match
    max_pixel = np.max(gt)  # Normalization factor
    return 20 * np.log10(max_pixel / np.sqrt(mse))

# Function to compute CCC
def compute_ccc(pred, gt):
    pred_mean, gt_mean = np.mean(pred), np.mean(gt)
    pred_var, gt_var = np.var(pred), np.var(gt)
    covariance = np.mean((pred - pred_mean) * (gt - gt_mean))
    return (2 * covariance) / (pred_var + gt_var + (pred_mean - gt_mean) ** 2)

# Model Evaluation
def evaluate_model(model, test_loader, device):
    model.eval()  # Set model to evaluation mode
    mse_cbf, mse_att = [], []
    psnr_cbf, psnr_att = [], []
    ssim_cbf, ssim_att = [], []
    ccc_cbf, ccc_att = [], []

    with torch.no_grad():  # No gradient calculation needed
        for pwis, gt in test_loader:
            pwis, gt = pwis.to(device), gt.to(device)
            predicted_cbf, predicted_att = model(pwis)

            # Convert tensors to numpy arrays
            predicted_cbf_np = predicted_cbf.cpu().numpy()
            predicted_att_np = predicted_att.cpu().numpy()
            gt_cbf_np = gt[:, 0].cpu().numpy()
            gt_att_np = gt[:, 1].cpu().numpy()

            # Compute metrics for each item in batch
            for i in range(predicted_cbf_np.shape[0]):
                pred_cbf_i = predicted_cbf_np[i]
                pred_att_i = predicted_att_np[i]
                gt_cbf_i = gt_cbf_np[i]
                gt_att_i = gt_att_np[i]

                # MSE
                mse_cbf.append(np.mean((pred_cbf_i - gt_cbf_i) ** 2))
                mse_att.append(np.mean((pred_att_i - gt_att_i) ** 2))

                # PSNR
                psnr_cbf.append(compute_psnr(pred_cbf_i, gt_cbf_i))
                psnr_att.append(compute_psnr(pred_att_i, gt_att_i))

                # SSIM (handle flat images)
                ssim_cbf.append(
                    ssim(pred_cbf_i, gt_cbf_i, data_range=np.ptp(gt_cbf_i) or 1)
                )
                ssim_att.append(
                    ssim(pred_att_i, gt_att_i, data_range=np.ptp(gt_att_i) or 1)
                )

                # CCC
                ccc_cbf.append(compute_ccc(pred_cbf_i, gt_cbf_i))
                ccc_att.append(compute_ccc(pred_att_i, gt_att_i))

    # Aggregate and print metrics
    print(f"CBF MSE: {np.mean(mse_cbf):.4f}")
    print(f"ATT MSE: {np.mean(mse_att):.4f}")
    print(f"CBF PSNR: {np.mean(psnr_cbf):.2f} dB")
    print(f"ATT PSNR: {np.mean(psnr_att):.2f} dB")
    print(f"CBF SSIM: {np.mean(ssim_cbf):.4f}")
    print(f"ATT SSIM: {np.mean(ssim_att):.4f}")
    print(f"CBF CCC: {np.mean(ccc_cbf):.4f}")
    print(f"ATT CCC: {np.mean(ccc_att):.4f}")

    # Optional: return metrics if needed for logging or saving
    return {
        "mse_cbf": np.mean(mse_cbf),
        "mse_att": np.mean(mse_att),
        "psnr_cbf": np.mean(psnr_cbf),
        "psnr_att": np.mean(psnr_att),
        "ssim_cbf": np.mean(ssim_cbf),
        "ssim_att": np.mean(ssim_att),
        "ccc_cbf": np.mean(ccc_cbf),
        "ccc_att": np.mean(ccc_att),
    }

# Run Evaluation
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
evaluate_model(dino_3d, test_loader, device)

import matplotlib.pyplot as plt
import numpy as np
import torch

def plot_cb_att_maps_coronal_horizontal(model, test_loader, device="cuda", slice_idx=23, sample_idx=1):
    model.eval()
    model.to(device)

    with torch.no_grad():
        for pwis, gt in test_loader:
            pwis = pwis.to(device)
            gt = gt.cpu().numpy()  # [B, 2, H, W, D]

            predicted_cbf, predicted_att = model(pwis)
            predicted_cbf = predicted_cbf.cpu().numpy()  # [B, H, W, D]
            predicted_att = predicted_att.cpu().numpy()  # [B, H, W, D]

            fig, axes = plt.subplots(1, 6, figsize=(20, 5))  # 1 row, 6 columns

            # Coronal view slice (fix Y axis = slice_idx)
            gt_cbf = gt[sample_idx, 0, slice_idx, :, :].T           # [W, D]
            pred_cbf = predicted_cbf[sample_idx, slice_idx, :, :].T # [W, D]
            err_cbf = np.abs(gt_cbf - pred_cbf)

            gt_att = gt[sample_idx, 1, slice_idx, :, :].T
            pred_att = predicted_att[sample_idx, slice_idx, :, :].T
            err_att = np.abs(gt_att - pred_att)

            # CBF row
            axes[0].imshow(gt_cbf.T, cmap="jet", origin="lower")
            axes[0].set_title("GT CBF")

            axes[1].imshow(pred_cbf.T, cmap="jet", origin="lower")
            axes[1].set_title("Predicted CBF")

            axes[2].imshow(err_cbf.T, cmap="hot", origin="lower")
            axes[2].set_title("CBF Error")

            # ATT row
            axes[3].imshow(gt_att.T, cmap="jet", origin="lower")
            axes[3].set_title("GT ATT")

            axes[4].imshow(pred_att.T, cmap="jet", origin="lower")
            axes[4].set_title("Predicted ATT")

            axes[5].imshow(err_att.T, cmap="hot", origin="lower")  # Transpose to make horizontal
            axes[5].set_title("ATT Error")

            for ax in axes.ravel():
                ax.axis("off")

            plt.tight_layout()
            plt.show()
            break  # only one batch

# Example call
plot_cb_att_maps_coronal_horizontal(dino_3d, test_loader, slice_idx=31)

import matplotlib.pyplot as plt
import numpy as np
import torch

def plot_cb_att_maps_coronal(model, test_loader, device="cuda", slice_idx=31, sample_idx=0):
    model.eval()
    model.to(device)

    with torch.no_grad():
        for pwis, gt in test_loader:
            pwis = pwis.to(device)
            gt = gt.cpu().numpy()  # [B, 2, H, W, D]

            predicted_cbf, predicted_att = model(pwis)
            predicted_cbf = predicted_cbf.cpu().numpy()  # [B, H, W, D]
            predicted_att = predicted_att.cpu().numpy()  # [B, H, W, D]

            fig, axes = plt.subplots(2, 3, figsize=(14, 9))

            # Coronal view → fix Y axis → slice along axis=1
            gt_cbf = gt[sample_idx, 0, slice_idx, :, :]         # [W, D]
            pred_cbf = predicted_cbf[sample_idx, slice_idx, :, :]  # [W, D]
            err_cbf = np.abs(gt_cbf - pred_cbf)

            axes[0, 0].imshow(gt_cbf.T, cmap="gray", origin='lower')
            axes[0, 0].set_title("Ground Truth CBF (Coronal)")

            axes[0, 1].imshow(pred_cbf.T, cmap="gray", origin='lower')
            axes[0, 1].set_title("Predicted CBF (Coronal)")

            axes[0, 2].imshow(err_cbf.T, cmap="hot", origin='lower')
            axes[0, 2].set_title("CBF Error Map")

            # ATT - Coronal
            gt_att = gt[sample_idx, 1, slice_idx, :, :]
            pred_att = predicted_att[sample_idx, slice_idx, :, :]
            err_att = np.abs(gt_att - pred_att)

            axes[1, 0].imshow(gt_att.T, cmap="gray", origin='lower')
            axes[1, 0].set_title("Ground Truth ATT (Coronal)")

            axes[1, 1].imshow(pred_att.T, cmap="gray", origin='lower')
            axes[1, 1].set_title("Predicted ATT (Coronal)")

            axes[1, 2].imshow(err_att.T, cmap="hot", origin='lower')
            axes[1, 2].set_title("ATT Error Map")

            for ax in axes.ravel():
                ax.axis("off")

            plt.tight_layout()
            plt.show()
            break

# Call the coronal view function
plot_cb_att_maps_coronal(dino_3d, test_loader, slice_idx=31)

import matplotlib.pyplot as plt
import numpy as np
import torch

def plot_cb_att_maps_coronal_horizontal(model, test_loader, device="cuda", slice_idx=31, sample_idx=0):
    model.eval()
    model.to(device)

    with torch.no_grad():
        for pwis, gt in test_loader:
            pwis = pwis.to(device)
            gt = gt.cpu().numpy()  # [B, 2, H, W, D]

            predicted_cbf, predicted_att = model(pwis)
            predicted_cbf = predicted_cbf.cpu().numpy()  # [B, H, W, D]
            predicted_att = predicted_att.cpu().numpy()  # [B, H, W, D]

            fig, axes = plt.subplots(1, 6, figsize=(20, 5))  # 1 row, 6 columns

            # Coronal view slice (fix Y axis = slice_idx)
            gt_cbf = gt[sample_idx, 0, slice_idx, :, :]           # [W, D]
            pred_cbf = predicted_cbf[sample_idx, slice_idx, :, :] # [W, D]
            err_cbf = np.abs(gt_cbf - pred_cbf)

            gt_att = gt[sample_idx, 1, slice_idx, :, :]
            pred_att = predicted_att[sample_idx, slice_idx, :, :]
            err_att = np.abs(gt_att - pred_att)

            # CBF row
            axes[0].imshow(gt_cbf.T, cmap="hot", origin="lower")
            axes[0].set_title("GT CBF")

            axes[1].imshow(pred_cbf.T, cmap="hot", origin="lower")
            axes[1].set_title("Predicted CBF")

            axes[2].imshow(err_cbf.T, cmap="hot", origin="lower")
            axes[2].set_title("CBF Error")

            # ATT row
            axes[3].imshow(gt_att.T, cmap="hot", origin="lower")
            axes[3].set_title("GT ATT")

            axes[4].imshow(pred_att.T, cmap="", origin="lower")
            axes[4].set_title("Predicted ATT")

            axes[5].imshow(err_att.T, cmap="hot", origin="lower")
            axes[5].set_title("ATT Error")

            for ax in axes.ravel():
                ax.axis("off")

            plt.tight_layout()
            plt.show()
            break  # only one batch

# Example call
plot_cb_att_maps_coronal_horizontal(dino_3d, test_loader, slice_idx=31)

import matplotlib.pyplot as plt
import numpy as np
import torch

def visualize_brain_views(model, test_loader, device="cuda", sample_idx=0, slice_index=40):
    model.eval()
    model.to(device)

    with torch.no_grad():
        for pwis, gt in test_loader:
            pwis = pwis.to(device)
            gt = gt.cpu().numpy()

            predicted_cbf, predicted_att = model(pwis)
            predicted_cbf = predicted_cbf.cpu().numpy()
            predicted_att = predicted_att.cpu().numpy()

            # Choose subject in batch
            idx = sample_idx

            # Extract individual volumes
            cbf_gt = gt[idx, 0]          # Shape: [H, W, D]
            cbf_pred = predicted_cbf[idx]

            att_gt = gt[idx, 1]
            att_pred = predicted_att[idx]

            # Slices: coronal (Y-axis), sagittal (X-axis), axial (Z-axis)
            coronal_cbf = cbf_pred[slice_index, :, :]
            sagittal_cbf = cbf_pred[:, slice_index, :]
            axial_cbf = cbf_pred[:, :, slice_index]

            coronal_att = att_pred[slice_index, :, :]
            sagittal_att = att_pred[:, slice_index, :]
            axial_att = att_pred[:, :, slice_index]

            # Plot CBF views
            fig, axes = plt.subplots(2, 3, figsize=(15, 8))
            axes[0, 0].imshow(coronal_cbf, cmap='gray');  axes[0, 0].set_title("CBF Coronal")
            axes[0, 1].imshow(sagittal_cbf, cmap='gray'); axes[0, 1].set_title("CBF Sagittal")
            axes[0, 2].imshow(axial_cbf, cmap='gray');    axes[0, 2].set_title("CBF Axial")

            # Plot ATT views
            axes[1, 0].imshow(coronal_att, cmap='gray');  axes[1, 0].set_title("ATT Coronal")
            axes[1, 1].imshow(sagittal_att, cmap='gray'); axes[1, 1].set_title("ATT Sagittal")
            axes[1, 2].imshow(axial_att, cmap='gray');    axes[1, 2].set_title("ATT Axial")

            for ax in axes.flat:
                ax.axis('off')

            plt.tight_layout()
            plt.show()
            break  # Only process one batch

# Call it like this:
visualize_brain_views(dino_3d, test_loader, device="cuda", sample_idx=0, slice_index=23)

!pip install pytorch-msssim

import matplotlib.pyplot as plt
import numpy as np
import torch

def plot_all_slices(pwis, subject_idx=0):
    """ Function to plot all slices of the input PWI image with slice numbers """

    # Convert PWI tensor to numpy
    input_pwi = pwis[subject_idx].cpu().numpy()

    # Average across the 3 PWI channels
    input_pwi_avg = np.mean(input_pwi, axis=0)  # Averaging along the last axis

    num_slices = input_pwi_avg.shape[-1]  # Total number of slices (Z-dimension)

    cols = 8  # Number of columns in the grid
    rows = int(np.ceil(num_slices / cols))  # Compute number of rows dynamically

    fig, axes = plt.subplots(rows, cols, figsize=(15, 2 * rows))

    for i in range(rows * cols):
        ax = axes[i // cols, i % cols]  # Get correct subplot

        if i < num_slices:
            vmin, vmax = np.percentile(input_pwi_avg[:, :, i], [2, 98])  # Normalize contrast
            ax.imshow(input_pwi_avg[:, :, i], cmap="gray", vmin=vmin, vmax=vmax)
            ax.set_title(f"Slice {i}", fontsize=8)
        else:
            ax.axis("off")  # Hide unused subplots

        ax.axis("off")  # Remove axis ticks

    plt.tight_layout()
    plt.show()

# Example Usage
with torch.no_grad():
    for pwis, _ in test_loader:  # Get a batch of data
        plot_all_slices(pwis, subject_idx=0)  # Visualize all slices of first subject in batch
        break  # Only process first batch

print(pwi_data.shape)  # Should be (D, H, W)
print(np.min(pwi_data), np.max(pwi_data))  # Check intensity range

